{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task 3: Prediction with Machine Learning \n\n### Table of Contents\n\n1. [Introduction](#introduction)\n2. [Data Preprocessing](#data-preprocessing)\n3. [Training Machine Learning Models](#machine-learning)\n    - [Feature Selection](#feature-selection)\n3. [Summary](#summary)\n\n    "},{"metadata":{},"cell_type":"markdown","source":"## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n\nMachine learning will be used to predict house price with the Ames Housing dataset, which has been cleaning during data wrangling and exploratory data analysis (EDA). The first step of this project is to preprocess data based on the insights discovered during exploratory data analysis. Subsequently, the preprocessed data will be used to train various machine learning models such as neural network, XGBoost, linear regression and etc in order to predict house price.\n\n## Data Preprocessing <a class=\"anchor\" id=\"data-preprocessing\"></a>\n\nTo preprocess the data, the following steps will be carried out in a pipeline.\n\n1. Split data into k-folds for k-folds cross validation\n2. For each fold,\n    1. Use ordinal encoding to encode ordinal variables\n    2. Use one-hot encoding to encode nominal variables with small cardinality and that seem to have substantial impact of house price (in order to retain all the information)\n    3. Use hash encoding to encode nominal variables with high cardinality\n    4. Perform standardization\n    5. Use k-nearest neighbors (KNN) imputer to impute missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport pickle\nfrom sklearn.model_selection import KFold, ParameterSampler, ParameterGrid\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nimport category_encoders as ce\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom scipy.stats import uniform\nimport xgboost","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class myOrdinalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode ordinal features, which may contain NaNs, as a 2D array\n        \n    Parameters\n    ----------      \n    unknown_value: int, default=0\n        Unknown value to use for unknown categorical feature which is not seen in training data\n    \"\"\"\n    \n    \n    def __init__( self, unknown_value=0):  \n        self.ordinal_encoder = None\n        self.categories = []\n        self.nan_replacement = None\n        self.unknown_value = unknown_value\n\n        \n    def fit( self, X, y = None ):\n        \"\"\"\n        Fit myOrdinalEncoder to X.\n    \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"      \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        # Replace NaNs with replacement before fitting to avoid errors\n        self.nan_replacement = X_copy.mode(dropna=True).iloc[0, :]\n        X_copy = X_copy.fillna(self.nan_replacement)\n        \n        for col in X_copy:\n            self.categories.append(list(X_copy[col].cat.categories))\n\n        # Fit ordinal encoder\n        self.ordinal_encoder = OrdinalEncoder(categories=self.categories, handle_unknown='use_encoded_value', unknown_value=np.nan)\n        self.ordinal_encoder.fit(X_copy);\n        \n        return self\n\n    def transform(self, X): \n        \"\"\"\n        Transform X using ordinal encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n          \n        # Create a numpy array that defines the locations of NaNs in the dataframe after hash encoding\n        nan_location_arr = X_copy.to_numpy()\n        nan_location_arr = pd.isnull(nan_location_arr)\n\n        # Replace NaNs with replacement before fitting to avoid errors\n        X_out = X_copy.fillna(self.nan_replacement)\n        \n        # Transform each feature\n        X_out = self.ordinal_encoder.transform(X_out)\n        \n        # Reconvert back values into NaNs\n        X_out[nan_location_arr] = np.nan\n        \n        return X_out\n        \n#         X_out = self.ordinal_encoder.transform(X_copy)\n    \n        return X_copy\n    \n    \nclass myOneHotEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode categorical features, which may contain NaNs, as a one-hot numeric array\n    \"\"\"\n    def __init__(self):\n        # Initialize one-hot encoder\n        self.one_hot_encoder = OneHotEncoder(drop='first', sparse=False)\n        self.nan_replacement = None\n        self.one_hot_length = []\n\n    def fit(self, X, y = None ):\n        \"\"\"\n        Fit myOneHotEncoder to X.\n    \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to train the encoder\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        # Replace NaNs with replacement before fitting to avoid errors\n        self.nan_replacement = X_copy.mode(dropna=True).iloc[0, :]\n        X_copy = X_copy.fillna(self.nan_replacement)\n        \n        # Fit one-hot encoder\n        self.one_hot_encoder.fit(X_copy)\n        \n        # Get the length of one-hot encoding for each feature\n        for category in self.one_hot_encoder.categories_:\n            # Minus length with 1 because of drop='first' in OneHotEncoder\n            self.one_hot_length.append(len(category) - 1) \n            \n        return self\n\n    \n    def transform(self, X):\n        \"\"\"\n        Transform X using one-hot encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        # Replace unknown value with NaN\n        for i, category in enumerate(self.one_hot_encoder.categories_):\n            X_copy.loc[~X_copy.loc[:, X_copy.columns[i]].isin(category),  X_copy.columns[i]] = np.nan\n\n        # Create a numpy array that defines the locations of NaNs in the dataframe after one-hot encoding\n        nan_location_arr = X_copy.to_numpy()\n        nan_location_arr = np.repeat(pd.isnull(nan_location_arr), repeats=self.one_hot_length, axis=1)\n        \n        # Replace NaNs with replacement before fitting to avoid errors\n        X_out = X_copy.fillna(self.nan_replacement)\n        \n        # Transform each feature\n        X_out = self.one_hot_encoder.transform(X_out)\n        \n        # Reconvert back values into NaNs\n        X_out[nan_location_arr] = np.nan\n        \n        return X_out\n    \n    \nclass myHashingEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode nominal features, which may contain NaNs, as a 2D array\n        \n    Parameters\n    ----------\n    n_bits: int, default=8\n        Number of bits used to represent each nominal feature\n    \"\"\"\n    \n    \n    def __init__(self, n_bits=8):\n        self.n_bits = n_bits\n        \n        # Initialize hashing encoder\n        self.hashing_encoder = ce.hashing.HashingEncoder(return_df=False, n_components=n_bits)\n        \n\n    def fit(self, X, y = None ):\n        \"\"\"\n        Ignored. This exists only for compatibility with \n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        return self\n    \n\n    def transform(self, X):\n        \"\"\"\n        Transform X using hash encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a numpy array that defines the locations of NaNs in the dataframe after hash encoding\n        nan_location_arr = X.to_numpy()\n        nan_location_arr = np.repeat(nan_location_arr, repeats=self.n_bits, axis=1)\n        \n        X_out = np.empty((X.shape[0],0))\n        \n        for col in X:\n            # Convert the data type of column into str if int or float\n            if(X[col].dtype == np.float64 or X[col].dtype == np.int64):\n                X[col] = X[col].astype(str)\n                \n            # Transform each feature and convert the data type into float for NaN\n            X_out = np.concatenate((X_out, self.hashing_encoder.fit_transform(X[col].to_numpy()).astype('float')), axis=1)\n            \n        # As hashing encoder will turn NaN into numeric array, reconvert back these values into NaNs\n        X_out[pd.isnull(nan_location_arr)] = np.nan\n        \n        return X_out\n    \n    \nclass hashAggregator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Combine all the 2D arrays of encoded using hash encoding after KNN imputation \n        \n    Parameters\n    ----------\n    n_feas: int\n        Number of features encoded using hash encoding \n    n_bits: int, default=8\n        Number of bits used to represent each nominal feature\n    \"\"\"\n    \n    \n    def __init__(self, n_feas, n_bits):\n        self.n_feas = n_feas\n        self.n_bits = n_bits\n        \n        \n    def fit(self, X, y = None ):\n        \"\"\"\n        Ignored. This exists only for compatibility with \n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        return self\n    \n\n    def transform(self, X):\n        \"\"\"\n        Combine all the 2D arrays of features encoded using hash encoding\n        \n        Parameters\n        ----------\n        X : array, shape [n_samples, n_features*n_bits]\n            The data to combine.\n        Returns\n        -------\n        X_out : 2D array\n            Combined input.\n        \"\"\"\n        X_copy = np.copy(X)\n\n        # Split input array into two array: one containing data encoded using hash encoding, another containing the rest\n        X_copy_non_hash, X_copy_hash = np.split(X_copy, [-self.n_feas*self.n_bits,], axis=1)\n        \n        # Combine all the 2D arrays of features encoded using hash encoding\n        X_copy_hash = X_copy_hash.reshape(X_copy_hash.shape[0], -1, self.n_bits).sum(1)\n        \n        # Concatenate hash and non-hash arrays\n        X_out = np.concatenate((X_copy_non_hash, X_copy_hash), axis=1)\n        \n        return X_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfolds_split(df, n_folds):\n    # Split data into K-folds\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n\n    kfold_df = {}\n\n    for i, (train_index, val_index) in enumerate(kf.split(df)):\n        kfold_df['Fold'+ str(i)] = df.loc[train_index, :].copy(), df.loc[val_index, :].copy()\n        \n    return kfold_df\n        \n        \ndef data_preprocessing(df, data_types, ordinal_var, n_bits=8):\n    \n    kfold_data = {}\n    \n    for key_fold, (df_train, df_val) in df.items():\n         \n        data_types_per_fold = data_types\n        ordinal_var_per_fold = ordinal_var\n        \n        for key in ordinal_var_per_fold:\n            # Remove categorical values from ordinal_var_per_fold that do not exist in training set to avoid data leakage\n            train_unique = df_train[key].unique()\n            ordinal_var_per_fold[key] = [cat_value for cat_value in ordinal_var_per_fold[key] if cat_value in train_unique]\n    \n            # Convert variables in training set into ordered categorical types\n            ordered_var = pd.api.types.CategoricalDtype(ordered = True, categories = ordinal_var_per_fold[key])\n            df_train.loc[:, key] = df_train.loc[:, key].astype(ordered_var)\n            \n        # Break nominal variables into two group: one-hot encoding or hash encoding\n        data_types_per_fold['nom_one_hot'] = []\n        data_types_per_fold['nom_hash'] = []\n\n        for var in data_types_per_fold['nom']:\n    \n            # Based on EDA conducted previously, Neighborhood, MSZoning, MasVnrType and Foundation seems to have \n            # substantial impact on SalePrice. Thus, one-hot encoding will be chosen over hash encoding for these\n            # variables, regardless of cardinality, because hash encoding will cause loss in information\n            if var in ['Neighborhood', 'MSZoning', 'MasVnrType', 'Foundation']:\n                data_types_per_fold['nom_one_hot'].append(var)\n        \n            else:\n                if len(df_train[var].unique()) <= 5 :\n                    # use one-hot encoding if the cardinality is small\n                    data_types_per_fold['nom_one_hot'].append(var)\n                else:\n                    # use one-hot encoding if the cardinality is big\n                    data_types_per_fold['nom_hash'].append(var) \n                    \n        # Define ColumnTransformer to transform columns with different methods\n        column_transformer = ColumnTransformer(transformers=[('dis_transformer', 'passthrough', data_types_per_fold['dis']),\n                                                             ('con_transformer', 'passthrough', data_types_per_fold['con']),\n                                                             ('ord_transformer', myOrdinalEncoder(), data_types_per_fold['ord']),\n                                                             ('nom_one_hot_transformer', myOneHotEncoder(), data_types_per_fold['nom_one_hot']),\n                                                             ('nom_hash_transformer', myHashingEncoder(n_bits = n_bits), data_types_per_fold['nom_hash']),\n                                                            ])\n\n\n        # Define pipeline for pre-processing data\n        preprocessor = Pipeline(steps=[('column_transformer', column_transformer),\n                                       ('standard_scaler', StandardScaler()),\n                                       ('imputer', KNNImputer(n_neighbors=5, weights='distance')),\n                                       ('hash_aggregator', hashAggregator(n_feas=len(data_types_per_fold['nom_hash']), n_bits=n_bits)),\n                                      ])\n\n        # Preprocess training and validation data\n        X_train = preprocessor.fit_transform(df_train)\n        X_val = preprocessor.transform(df_val)\n        y_train = df_train['SalePrice'].to_numpy()\n        y_val = df_val['SalePrice'].to_numpy()\n        \n        kfold_data[key_fold] = ((X_train, y_train), (X_val, y_val))\n        \n    return kfold_data\n\n\ndef data_preparation(filepath, n_folds=5, unwanted_vars_dict=None):  \n    \"\"\"\n    Split data into k-folds and pre-process them\n        \n    Parameters\n    ----------\n    filepath: string\n        Path to file storing data\n    n_folds: int\n        Number of folds for cross-validation\n    unwanted_vars_dict: dict\n        Unwanted variables among discrete, continuous, nominal and ordinal variables\n    \"\"\"\n    \n    # Load training and testing sets\n    df = pd.read_csv(filepath)\n\n    # Drop ID column\n    df.drop(['Id'], axis=1, inplace=True)\n\n    # Define discrete(dis), continuous(con), nominal(nom), ordinal(ord) variables exlcuding target variable, which is SalePrice\n    data_types_dict = {'nom': ['MSSubClass', 'MSZoning', 'Street', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n                               'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n                               'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'SaleType', 'SaleCondition'],\n                       'ord': ['LotShape', 'LandContour', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', \n                               'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', \n                               'GarageFinish', 'GarageQual', 'GarageCond'],\n                       'dis': ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                               'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold'],\n                       'con': ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n                               '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n                               '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n                      }\n    \n    # Remove unwanted variables if there is any\n    if unwanted_vars_dict is not None:\n        for data_type in unwanted_vars_dict.keys():\n            data_types_dict[data_type] = list(set(data_types_dict[data_type]) - set(unwanted_vars_dict[data_type]))\n    \n    # Define order of categorical values in each ordinal variables\n    ordinal_var_dict = {'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n                        'LandContour': ['Lvl', 'Bnk', 'HLS', 'Low'],\n                        'LandSlope': ['Sev', 'Mod', 'Gtl'],\n                        'OverallQual': list(range(0,11)),\n                        'OverallCond': list(range(0,11)),\n                        'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'BsmtQual': ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'BsmtCond': ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'BsmtExposure': ['NoBsmt', 'No', 'Mn', 'Av', 'Gd'],\n                        'BsmtFinType1': ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                        'BsmtFinType2': ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                        'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n                        'GarageFinish': ['NoGarage', 'Unf', 'RFn', 'Fin'],\n                        'GarageQual': ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                        'GarageCond': ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                       }\n    \n    # Remove ordinal variables from ordinal_var_dict\n    if unwanted_vars_dict is not None and unwanted_vars_dict.get('ord') is not None:\n        for unwanted_nom_var in unwanted_vars_dict.get('ord'):\n            ordinal_var_dict.pop(unwanted_nom_var)\n            \n    # Split data into n folds\n    kfolds_data = kfolds_split(df, n_folds)\n    \n    # Preprocess data\n    kfolds_data = data_preprocessing(kfolds_data, data_types_dict, ordinal_var_dict)\n    \n    return kfolds_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Machine Learning Models <a class=\"anchor\" id=\"machine-learning\"></a>\n\nVarious machine learning models are used to predict SalePrice. The evaluation metric used for evaluating machine learning models with k-folds cross validation is mean squared log error. Log error is chosen because there is a big variation in values of SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define URL of training set\ndirname = '/kaggle/input'\nsubdirname = 'dataset'\ntrain_filename = 'train_clean_EDA.csv'\ntrain_filepath = os.path.join(dirname, subdirname, train_filename)\n\n# Define number of folds for cross validation\nn_folds = 5\n\n# Define URL of preprocessed data file \ndirname = '/kaggle/input/dataset'\nk_folds_data_filename = 'k_folds_data.pkl'\nk_folds_data_filepath = os.path.join(dirname, k_folds_data_filename)\n\nif os.path.exists(k_folds_data_filepath):\n    # If file exists, then load data from the file\n    with open(k_folds_data_filepath, 'rb') as f:\n        k_folds_data = pickle.load(f)\nelse:\n    # Else, store the data in a file to avoid re-computation in the future\n    with open(os.path.join('/kaggle/working', k_folds_data_filename ), 'wb') as f:\n        k_folds_data = data_preparation(train_filepath, n_folds)\n        pickle.dump(k_folds_data, f)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(k_folds_data, models):\n    \n    if not isinstance(models, list):\n        models = [models]\n       \n    # Empty dataframe, to be used for storing cross-validation performance later\n    cv_performance = pd.DataFrame()\n    \n    best_model_performance = None\n    best_model_index = None\n    \n    for i, model in enumerate(models):\n        \n        # Empty list, to be used for storing mean squared log error (MSLE)\n        msles_train = []\n        msles_val = []\n        \n        for _, ((X_train, y_train), (X_val, y_val)) in k_folds_data.items():\n            # Define end-to-end machine learning pipeline\n            ml_model = TransformedTargetRegressor(regressor=model, func=np.log, inverse_func=np.exp)\n            \n            # Train model\n            ml_model.fit(X_train, y_train)\n            \n            # Predict with training and validation data\n            y_train_pred = ml_model.predict(X_train)\n            y_val_pred = ml_model.predict(X_val)\n            \n            # Compute root mean squared log error (MSLE) for training data\n            msles_train.append(np.round(np.sqrt(mean_squared_log_error(y_train, y_train_pred)), 6))\n            \n            # Compute root mean squared log error (MSLE) for validation data\n            msles_val.append(np.round(np.sqrt(mean_squared_log_error(y_val, y_val_pred)), 6))\n            \n        msles_train_mean = np.mean(msles_train)\n        msles_val_mean = np.mean(msles_val)\n        \n        # Compare performances of current model and the best model\n        if i == 0 or (msles_val_mean < best_model_performance):\n            best_model_performance = msles_val_mean\n            best_model_index = i+1\n        \n        # Compute mean of MLSEs after k-folds cross validation\n        msles_train.append(msles_train_mean)\n        msles_val.append(msles_val_mean)\n        \n        # Store MLSEs in the dataframe\n        cv_performance[str(model) + \"_train\"] = msles_train\n        cv_performance[str(model) + \"_val\"] = msles_val\n        \n        print('Finished training {}/{} models'.format(i, len(models)))\n\n    # Update index of the dataframe\n    cv_performance.index = [\"Fold \" + str(i) for i in range(len(k_folds_data))] + ['Mean']\n    \n    # Return best_model_index and a dataframe containing the training and validation performance of the best model\n    return best_model_index, cv_performance[cv_performance.columns[[best_model_index*2-2, best_model_index*2-1]]]","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for ridge regression model\nparam_grid = {'alpha':[0, 1e-1, 3e-1, 1, 3, 10, 30, 100, 300, 1000]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train ridge regression model\nmodels = [Ridge(alpha=val) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data, models)\ndf_performance","execution_count":4,"outputs":[{"output_type":"stream","text":"Finished training 0/10 models\nFinished training 1/10 models\nFinished training 2/10 models\nFinished training 3/10 models\nFinished training 4/10 models\nFinished training 5/10 models\nFinished training 6/10 models\nFinished training 7/10 models\nFinished training 8/10 models\nFinished training 9/10 models\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"        Ridge(alpha=30)_train  Ridge(alpha=30)_val\nFold 0               0.099624             0.126681\nFold 1               0.105371             0.104219\nFold 2               0.100818             0.131173\nFold 3               0.101336             0.120066\nFold 4               0.106405             0.099264\nMean                 0.102711             0.116281","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ridge(alpha=30)_train</th>\n      <th>Ridge(alpha=30)_val</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fold 0</th>\n      <td>0.099624</td>\n      <td>0.126681</td>\n    </tr>\n    <tr>\n      <th>Fold 1</th>\n      <td>0.105371</td>\n      <td>0.104219</td>\n    </tr>\n    <tr>\n      <th>Fold 2</th>\n      <td>0.100818</td>\n      <td>0.131173</td>\n    </tr>\n    <tr>\n      <th>Fold 3</th>\n      <td>0.101336</td>\n      <td>0.120066</td>\n    </tr>\n    <tr>\n      <th>Fold 4</th>\n      <td>0.106405</td>\n      <td>0.099264</td>\n    </tr>\n    <tr>\n      <th>Mean</th>\n      <td>0.102711</td>\n      <td>0.116281</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for lasso regression model\nparam_grid = {'alpha':[0, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train lasso regression model\nmodels = [Lasso(alpha=val, max_iter=1e7) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Elastic Net Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for elastic net regression model\nparam_grid = {'alpha':[1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1],             \n              'l1_ratio':[0, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1]}\nparam_list = list(ParameterSampler(param_grid, n_iter=30))\n\n# Train elastic net regression model\nmodels = []\nfor param in param_list:\n    models.append(ElasticNet(alpha=param['alpha'], l1_ratio=param['l1_ratio'], max_iter=1e7))\nbest_model_index, df_performance = train_model(k_folds_data, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting (XGBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_xgboost_model(k_folds_data, models):\n    \n    if not isinstance(models, list):\n        models = [models]\n       \n    # Empty dataframe, to be used for storing cross-validation performance later\n    cv_performance = pd.DataFrame()\n    \n    best_model_performance = None\n    best_model = None\n    best_model_index = None\n    \n    for i, model in enumerate(models):\n        \n        # Empty list, to be used for storing mean squared log error (MSLE)\n        msles_train = []\n        msles_val = []\n        \n        for _, ((X_train, y_train), (X_val, y_val)) in k_folds_data.items():\n            # Define end-to-end machine learning pipeline\n            ml_model = TransformedTargetRegressor(regressor=model, func=np.log, inverse_func=np.exp)\n            \n            # Train model\n            ml_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=['rmsle'], early_stopping_rounds=10, verbose=False)\n            \n            # Predict with training and validation data\n            y_train_pred = ml_model.predict(X_train)\n            y_val_pred = ml_model.predict(X_val)\n            \n            # Compute mean squared log error (MSLE) for training data\n            msles_train.append(np.round(np.sqrt(mean_squared_log_error(y_train, y_train_pred)), 6))\n            \n            # Compute mean squared log error (MSLE) for validation data\n            msles_val.append(np.round(np.sqrt(mean_squared_log_error(y_val, y_val_pred)), 6))\n            \n        msles_train_mean = np.mean(msles_train)\n        msles_val_mean = np.mean(msles_val)\n        \n        if i == 0 or (msles_val_mean < best_model_performance):\n            best_model_performance = msles_val_mean\n            best_model_index = i+1\n            best_model = ml_model\n        \n        # Compute mean of MLSEs after k-folds cross validation\n        msles_train.append(msles_train_mean)\n        msles_val.append(msles_val_mean)\n       \n        # Store MLSEs in the dataframe\n        cv_performance[str(model) + \"_train\"] = msles_train\n        cv_performance[str(model) + \"_val\"] = msles_val\n        \n        print('Finished training {}/{} models'.format(i, len(models)))\n\n    # Update index of the dataframe\n    cv_performance.index = [\"Fold \" + str(i) for i in range(len(k_folds_data))] + ['Average']\n    \n    return best_model, best_model_index, cv_performance[cv_performance.columns[[best_model_index*2-2, best_model_index*2-1]]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for XGBoost\nparam_grid = {'n_estimator':list(range(1500, 3500, 20)), \n              'max_depth':[3, 4, 5, 6, 7],\n              'learning_rate':[0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n              'reg_alpha':uniform(),\n              'reg_lambda':uniform(),\n             }\nparam_list = list(ParameterSampler(param_grid, n_iter=200))\n\n\n# Train XGBoost model\nmodels = []\nfor param in param_list:\n    models.append(xgboost.XGBRegressor(n_estimators=param['n_estimator'], \n                                       max_depth=param['max_depth'], \n                                       learning_rate=param['learning_rate'], \n                                       reg_alpha=param['reg_alpha'], \n                                       reg_lambda=param['reg_lambda'],\n                                       subsample= 0.7, colsample_bytree = 0.7, objective = 'reg:squarederror'))\n    \nbest_model, best_model_index, df_performance = train_xgboost_model(k_folds_data, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model for All Features\n\nThe following table summarizes the performance of each model:\n\n| Model* | Mean of training RMSLE | Mean of validation RMSLE |\n| --- | --- | --- |\n| Ridge Regression | 0.1027 | 0.1163 |\n| Lasso Regression | 0.1029 | 0.1159 |\n| ElasticNet Regression | 0.1043 | 0.1157 |\n| XGBoost | 0.0532 | 0.1182 |\n<br>\n\n\\* Best hyperparameter(s) for each model is listed below. <br>\nRidge Regression -> alpha = 30; <br>\nLasso Regression -> alpha = 0.001; <br>\nElasticNet Regression -> alpha = 0.03; l1_ratio = 0.03; <br>\nXGBoost -> n_estimator = 3080; max_depth = 5; learning_rate = 0.003; reg_alpha = 0.051221; reg_lambda = 0.058402; <br>\n\n\nAs shown in the table, all the machine learning models has very similar validation performance, with ElasticNet regression has the best validation performance."},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection<a class=\"anchor\" id=\"feature-selection\"></a>\n\nAfter trying to train the machine learning models with all the features, a subset of features will be selected to train the machine learning models. Based on the results of exploratory data analysis, several features were found to be worth dropping. Features that can be dropped because of their high correlation with another feature are GarageYrBuilt, TotRmsAbvGrd, GarageCars and TotalBsmtSF. Whereas, features that can be dropped because of their low correlation with SalePrice are ScreenPorch, MoSold, LowQualFinSF, 3SsnPorch, MiscVal, PoolArea, BsmtFinSF2, YrSold, BsmtHalfBath and Utilities. Two different sets of features will be attempted, and they are:\n\n1) All features excluding highly correlated features <br>\n2) All features excluding highly correlated features and features with low correlation with SalePrice\n\n### Set 1\n\nAll features excluding highly correlated features (GarageYrBuilt, TotRmsAbvGrd, GarageCars and TotalBsmtSF)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define URL of training set\ndirname = '/kaggle/input'\nsubdirname = 'dataset'\ntrain_filename = 'train_clean_EDA.csv'\ntrain_filepath = os.path.join(dirname, subdirname, train_filename)\n\n# Define number of folds for cross validation\nn_folds = 5\n\n# Define variables to be dropped for set 1\nunwanted_vars_dict = {'dis': ['GarageYrBlt', 'TotRmsAbvGrd', 'GarageCars'],\n                      'con': ['TotalBsmtSF']\n                     }\n\n# Define URL of preprocessed data file \ndirname = '/kaggle/input/dataset'\nk_folds_data_filename = 'k_folds_data_set_1.pkl'\nk_folds_data_filepath = os.path.join(dirname, k_folds_data_filename)\n\nif os.path.exists(k_folds_data_filepath):\n    # If file exists, then load data from the file\n    with open(k_folds_data_filepath, 'rb') as f:\n        k_folds_data_set1 = pickle.load(f)\nelse:\n    # Else, store the data in a file to avoid re-computation in the future\n    with open(os.path.join('/kaggle/working', k_folds_data_filename ), 'wb') as f:\n        k_folds_data_set1 = data_preparation(train_filepath, n_folds, unwanted_vars_dict)\n        pickle.dump(k_folds_data_set1, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for ridge regression model\nparam_grid = {'alpha':[0, 1e-1, 3e-1, 1, 3, 10, 30, 100, 300, 1000]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train ridge regression model\nmodels = [Ridge(alpha=val) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data_set1, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for lasso regression model\nparam_grid = {'alpha':[0, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train lasso regression model\nmodels = [Lasso(alpha=val, max_iter=1e7) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data_set1, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for elastic net regression model\nparam_grid = {'alpha':[1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1],             \n              'l1_ratio':[0, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1]}\nparam_list = list(ParameterSampler(param_grid, n_iter=30))\n\n# Train elastic net regression model\nmodels = []\nfor param in param_list:\n    models.append(ElasticNet(alpha=param['alpha'], l1_ratio=param['l1_ratio'], max_iter=1e7))\nbest_model_index, df_performance = train_model(k_folds_data_set1, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting (XGBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for XGBoost\nparam_grid = {'n_estimator':list(range(1500, 3500, 20)), \n              'max_depth':[3, 4, 5, 6, 7],\n              'learning_rate':[0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n              'reg_alpha':uniform(),\n              'reg_lambda':uniform(),\n             }\nparam_list = list(ParameterSampler(param_grid, n_iter=200))\n\n\n# Train XGBoost model\nmodels = []\nfor param in param_list:\n    models.append(xgboost.XGBRegressor(n_estimators=param['n_estimator'], \n                                       max_depth=param['max_depth'], \n                                       learning_rate=param['learning_rate'], \n                                       reg_alpha=param['reg_alpha'], \n                                       reg_lambda=param['reg_lambda'],\n                                       subsample= 0.7, colsample_bytree = 0.7, objective = 'reg:squarederror'))\n    \nbest_model, best_model_index, df_performance = train_xgboost_model(k_folds_data_set1, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model for Set 1 Features\n\nThe following table summarizes the performance of each model for set 1 features:\n\n| Model* | Mean of training RMSLE | Mean of validation RMSLE |\n| --- | --- | --- |\n| Ridge Regression | 0.1021 | 0.1164 |\n| Lasso Regression | 0.1033 | 0.1161 |\n| ElasticNet Regression | 0.1041 | 0.1160 |\n| XGBoost | 0.0634 | 0.1191 |\n<br>\n\n\\* Best hyperparameter(s) for each model is listed below. <br>\nRidge Regression -> alpha = 10; <br>\nLasso Regression -> alpha = 0.001; <br>\nElasticNet Regression -> alpha = 0.01; l1_ratio = 0.1; <br>\nXGBoost -> n_estimator = 1500; max_depth = 4; learning_rate = 0.03; reg_alpha = 0.003169; reg_lambda = 0.108066; <br>\n\nAs shown in the table, all the machine learning models have slightly worse performances when highly correlated features are removed (set 1) as compared to when all features are used. Same as before, ElasticNet regression has the best validation performance among all the models.\n\n### Set 2\n\nAll features excluding highly correlated features (GarageYrBuilt, TotRmsAbvGrd, GarageCars and TotalBsmtSF) and features with low correlation with SalePrice (ScreenPorch, MoSold, LowQualFinSF, 3SsnPorch, MiscVal, PoolArea, BsmtFinSF2, YrSold, BsmtHalfBath and Utilities)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define URL of training set\ndirname = '/kaggle/input'\nsubdirname = 'dataset'\ntrain_filename = 'train_clean_EDA.csv'\ntrain_filepath = os.path.join(dirname, subdirname, train_filename)\n\n# Define number of folds for cross validation\nn_folds = 5\n\n# Define variables to be dropped for set 2\nunwanted_vars_dict = {'nom': ['Utilities'],\n                      'dis': ['GarageYrBlt', 'TotRmsAbvGrd', 'GarageCars', 'MoSold', 'YrSold', 'BsmtHalfBath'],\n                      'con': ['TotalBsmtSF', 'ScreenPorch', 'LowQualFinSF', '3SsnPorch', 'MiscVal', 'PoolArea', 'BsmtFinSF2']\n                     }\n\n# Define URL of preprocessed data file \ndirname = '/kaggle/input/dataset'\nk_folds_data_filename = 'k_folds_data_set_2.pkl'\nk_folds_data_filepath = os.path.join(dirname, k_folds_data_filename)\n\nif os.path.exists(k_folds_data_filepath):\n    # If file exists, then load data from the file\n    with open(k_folds_data_filepath, 'rb') as f:\n        k_folds_data_set2 = pickle.load(f)\nelse:\n    # Else, store the data in a file to avoid re-computation in the future\n    with open(os.path.join('/kaggle/working', k_folds_data_filename ), 'wb') as f:\n        k_folds_data_set2 = data_preparation(train_filepath, n_folds, unwanted_vars_dict)\n        pickle.dump(k_folds_data_set2, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for lasso regression model\nparam_grid = {'alpha':[1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train lasso regression model\nmodels = [Lasso(alpha=val, max_iter=1e7) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data_set2, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for ridge regression model\nparam_grid = {'alpha':[0, 1e-1, 3e-1, 1, 3, 10, 30, 100, 300, 1000]}\nparam_list = list(ParameterGrid(param_grid))\n\n# Train ridge regression model\nmodels = [Ridge(alpha=val) for param in param_list for _, val in param.items() ]\nbest_model_index, df_performance = train_model(k_folds_data_set2, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for elastic net regression model\nparam_grid = {'alpha':[1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1],             \n              'l1_ratio':[1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1]}\nparam_list = list(ParameterSampler(param_grid, n_iter=30))\n\n# Train elastic net regression model\nmodels = []\nfor param in param_list:\n    models.append(ElasticNet(alpha=param['alpha'], l1_ratio=param['l1_ratio'], max_iter=1e7))\nbest_model_index, df_performance = train_model(k_folds_data_set2, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting (XGBoost)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter grid for XGBoost\nparam_grid = {'n_estimator':list(range(1500, 3500, 20)), \n              'max_depth':[3, 4, 5, 6, 7],\n              'learning_rate':[0.001, 0.003, 0.01, 0.03, 0.1, 0.3],\n              'reg_alpha':uniform(),\n              'reg_lambda':uniform(),\n             }\nparam_list = list(ParameterSampler(param_grid, n_iter=200))\n\n\n# Train XGBoost model\nmodels = []\nfor param in param_list:\n    models.append(xgboost.XGBRegressor(n_estimators=param['n_estimator'], \n                                       max_depth=param['max_depth'], \n                                       learning_rate=param['learning_rate'], \n                                       reg_alpha=param['reg_alpha'], \n                                       reg_lambda=param['reg_lambda'],\n                                       subsample= 0.7, colsample_bytree = 0.7, objective = 'reg:squarederror'))\n    \nbest_model, best_model_index, df_performance = train_xgboost_model(k_folds_data_set2, models)\ndf_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Model for Set 2 Features\n\nThe following table summarizes the performance of each model for set 2 features:\n\n| Model* | Mean of training RMSLE | Mean of validation RMSLE |\n| --- | --- | --- |\n| Ridge Regression | 0.1038 | 0.1161 |\n| Lasso Regression | 0.1049 | 0.1162 |\n| ElasticNet Regression | 0.1040 | 0.1161 |\n| XGBoost | 0.0357 | 0.1197 |\n<br>\n\n\\* Best hyperparameter(s) for each model is listed below. <br>\nRidge Regression -> alpha = 10; <br>\nLasso Regression -> alpha = 0.001; <br>\nElasticNet Regression -> alpha = 0.01 ; l1_ratio = 0.0003; <br>\nXGBoost -> n_estimator = 2460; max_depth = 7; learning_rate = 0.01; reg_alpha = 0.045753; reg_lambda = 0.305358; <br>\n\nAs shown in the table, all the machine learning models, expect ridge regression, have slightly worse performances when highly correlated features and features with high correlation with SalePrice (set 2) are removed, as compared to when all features or set 1 features are used. On the other hand, ridge regression has the best performance when set 2 features are used, as compared to when all features or set 1 features are used\nElasticNet regression and Ridge regression have the same validation performance, which is the best among all the models"},{"metadata":{},"cell_type":"markdown","source":"## Summary<a class=\"anchor\" id=\"summary\"></a>\n\nTo encode categorical variables, ordinal encoding was used for encoding ordinal variables, whereas one-hot encoding was used for encoding nominal variables with small cardinality and that seem to have substantial impact of house price (in order to retain all the information). For nominal variables with high cardinality, hash encoding was used. After encoding was completed, standardization was performed on all the features. The final preprocessing step was to use k-nearest neighbors (KNN) imputer to impute missing values.\n\nIt was found that the best model performance was obtained when all features were used, as compared to when set 1 and set 2 features were used. Using 5-folds cross validation, the best model found in this project is ElasticNet regression (alpha = 0.03; l1_ratio =0.03), which has a validation root mean square log error (RMSLE) of 0.1157.\n\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}