{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task 3: Prediction with Machine Learning \n\n### Table of Contents\n\n1. [Introduction](#introduction)\n2. [Data Preproceesing](#data-preprocessing)\n3. [Machine Learning](#machine-learning)"},{"metadata":{},"cell_type":"markdown","source":"## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n\nMachine learning will be applied to predict house price with the Ames Housing dataset, which has been cleaning during data wrangling and exploratory data analysis (EDA). The first steps of this project is to preprocess data based on the insights discovered during exploratory data analysis. Subsequently, the preprocessed data will be used to train various machine learning models such as neural network, XGBoost, linear regression and etc in order to predict house price.\n\n## Data Preprocessing <a class=\"anchor\" id=\"data-preprocessing\"></a>\n\nTo preprocess the data, the following steps will be carried out in a pipeline.\n\n1. Split data into training data and validation data with a ratio of 80:20\n2. Use ordinal encoding to encode ordinal variables\n3. Use one-hot encoding to encode nominal variables with small cardinality and that seem to have substantial impact of house price (in order to retain all the information).\n4. Use hash encoding to encode nominal variables with high cardinality\n5. Perform standardization\n6. Use k-nearest neighbors (KNN) imputer to impute missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nimport category_encoders as ce\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import KNNImputer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d695d26e-de2f-4c54-a645-de7702239cb1","_cell_guid":"a4867761-e6b0-4dcd-a3cd-594c7062b282","trusted":true},"cell_type":"code","source":"# Define URL of training set\ndirname = '/kaggle/input'\nsubdirname = 'dataset'\ntrain_filename = 'train_clean_EDA.csv'\ntrain_filepath = os.path.join(dirname, subdirname, train_filename)\n\n# Load training and testing sets\ndf = pd.read_csv(train_filepath)\n\n# Drop ID column\ndf.drop(['Id'], axis=1, inplace=True)\n\n# Split data into training and validation data with the ratio of 80:20\ndf_train, df_val = train_test_split(df, train_size=0.8, random_state=0)\n\n# Define discrete(dis), continuous(con), nominal(nom), ordinal(ord) variables\ndata_types_dict = {'nom': ['MSSubClass', 'MSZoning', 'Street', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n                           'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n                           'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'SaleType', 'SaleCondition'],\n                   \n                   'ord': ['LotShape', 'LandContour', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', \n                           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', \n                           'GarageFinish', 'GarageQual', 'GarageCond'],\n                   \n                   'dis': ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                           'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold'],\n                   \n                   'con': ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n                           '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n                           '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'SalePrice']\n                  }\n\n# Define order of categorical values in each ordinal variables\nordinal_var_dict = {'LotShape': ['IR3', 'IR2', 'IR1', 'Reg'],\n                    'LandContour': ['Lvl', 'Bnk', 'HLS', 'Low'],\n                    'LandSlope': ['Sev', 'Mod', 'Gtl'],\n                    'OverallQual': list(range(0,11)),\n                    'OverallCond': list(range(0,11)),\n                    'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtQual': ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtCond': ['NoBsmt', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'BsmtExposure': ['NoBsmt', 'No', 'Mn', 'Av', 'Gd'],\n                    'BsmtFinType1': ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                    'BsmtFinType2': ['NoBsmt', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n                    'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n                    'GarageFinish': ['NoGarage', 'Unf', 'RFn', 'Fin'],\n                    'GarageQual': ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                    'GarageCond': ['NoGarage', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n                   }\n\nfor key in ordinal_var_dict:\n    # Remove categorical values from ordinal_var_dict that do not exist in training set to avoid data leakage\n    train_unique = df_train[key].unique()\n    ordinal_var_dict[key] = [cat_value for cat_value in ordinal_var_dict[key] if cat_value in train_unique]\n    \n    # Convert variables in training set into ordered categorical types\n    ordered_var = pd.api.types.CategoricalDtype(ordered = True, categories = ordinal_var_dict[key])\n    df_train.loc[:, key] = df_train.loc[:, key].astype(ordered_var)\n    \n\n# Define discrete(dis), continuous(con), nominal(nom), ordinal(ord) variables exlcuding target variable, which is SalePrice\ndata_types_dict = {'nom': ['MSSubClass', 'MSZoning', 'Street', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n                           'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n                           'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'SaleType', 'SaleCondition'],\n                   \n                   'ord': ['LotShape', 'LandContour', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', \n                           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', \n                           'GarageFinish', 'GarageQual', 'GarageCond'],\n                   \n                   'dis': ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                           'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold'],\n                   \n                   'con': ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n                           '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n                           '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n                  }\n\n# Break nominal variables into two group: one-hot encoding or hash encoding\ndata_types_dict['nom_one_hot'] = []\ndata_types_dict['nom_hash'] = []\n\nfor var in data_types_dict['nom']:\n    \n    # Based on EDA conducted previously, Neighborhood, MSZoning, MasVnrType and Foundation seems to have \n    # substantial impact on SalePrice. Thus, one-hot encoding will be chosen over hash encoding for these\n    # variables, regardless of cardinality, because hash encoding will cause loss in information\n    if var in ['Neighborhood', 'MSZoning', 'MasVnrType', 'Foundation']:\n        data_types_dict['nom_one_hot'].append(var)\n        \n    else:\n        if len(df_train[var].unique()) <= 5 :\n            # use one-hot encoding if the cardinality is small\n            data_types_dict['nom_one_hot'].append(var)\n        else:\n            # use one-hot encoding if the cardinality is big\n            data_types_dict['nom_hash'].append(var) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class myOrdinalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode ordinal features, which may contain NaNs, as a 2D array\n        \n    Parameters\n    ----------\n    categories: dict\n        A dictionary of unique categorical values for each ordinal variable\n        \n    unknown_value: int, default=0\n        Unknown value to use for unknown categorical feature which is not seen in training data\n    \"\"\"\n    \n    \n    def __init__( self, categories={}, unknown_value=0):  \n        self.categories = categories\n        self.unknown_value = unknown_value\n\n        \n    def fit( self, X, y = None ):\n        \"\"\"\n        Fit myOrdinalEncoder to X.\n    \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        if not self.categories:\n            for col in X:\n                self.categories[col] = list(X[col].cat.categories)\n                \n        return self\n\n    def transform(self, X): \n        \"\"\"\n        Transform X using ordinal encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_copy : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        for col in X_copy:\n            \n            # Unconvert CategoricalDtype back to the original data type\n            X_copy[col] = np.where(X_copy[col].isnull(), np.nan, X_copy[col].astype(type(self.categories[col][0])))\n\n            # Set unknown categorical feature (except NaN) to unknown_value\n            X_copy.loc[~X_copy[col].isin(self.categories[col]) & X_copy[col].notnull(), col] = self.unknown_value\n \n            # Transform each feature to ordinal codes\n            for i, category in enumerate(self.categories[col]):\n                X_copy[col].replace(category, i+1, inplace=True)\n    \n        return X_copy\n    \n    \nclass myOneHotEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode categorical features, which may contain NaNs, as a one-hot numeric array\n    \"\"\"\n    def __init__(self):\n        # Initialize one-hot encoder\n        self.one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n        self.nan_replacement = None\n        self.one_hot_length = []\n\n    def fit(self, X, y = None ):\n        \"\"\"\n        Fit myOneHotEncoder to X.\n    \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to train the encoder\n        y : None\n            Ignored. This parameter exists only for compatibility with\n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        # Replace NaNs with replacement before fitting to avoid errors\n        self.nan_replacement = X_copy.mode(dropna=True).iloc[0, :]\n        X_copy = X_copy.fillna(self.nan_replacement)\n        \n        # Fit one-hot encoder\n        self.one_hot_encoder.fit(X_copy)\n        \n        # Get the length of one-hot encoding for each feature\n        for category in self.one_hot_encoder.categories_:\n            self.one_hot_length.append(len(category)) \n            \n        return self\n\n    \n    def transform(self, X):\n        \"\"\"\n        Transform X using one-hot encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a copy of X\n        X_copy = X.copy()\n        \n        # Create a numpy array that defines the locations of NaNs in the dataframe after hash encoding\n        nan_location_arr = X.to_numpy()\n        nan_location_arr = np.repeat(nan_location_arr, repeats=self.one_hot_length, axis=1)\n\n        # Replace NaNs with replacement before fitting to avoid errors\n        X_out = X_copy.fillna(self.nan_replacement)\n        \n        # Transform each feature\n        X_out = self.one_hot_encoder.transform(X_out).toarray()\n        \n        # Reconvert back values into NaNs\n        X_out[pd.isnull(nan_location_arr)] = np.nan\n        \n        return X_out\n    \n    \nclass myHashingEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Encode nominal features, which may contain NaNs, as a 2D array\n        \n    Parameters\n    ----------\n    n_bits: int, default=8\n        Number of bits used to represent each nominal feature\n    \"\"\"\n    \n    \n    def __init__(self, n_bits=8):\n        self.n_bits = n_bits\n        \n        # Initialize hashing encoder\n        self.hashing_encoder = ce.hashing.HashingEncoder(return_df=False, n_components=n_bits)\n        \n\n    def fit(self, X, y = None ):\n        \"\"\"\n        Ignored. This exists only for compatibility with \n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        return self\n    \n\n    def transform(self, X):\n        \"\"\"\n        Transform X using hash encoding.\n        \n        Parameters\n        ----------\n        X : DataFrame, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : 2D array\n            Transformed input.\n        \"\"\"\n        \n        # Create a numpy array that defines the locations of NaNs in the dataframe after hash encoding\n        nan_location_arr = X.to_numpy()\n        nan_location_arr = np.repeat(nan_location_arr, repeats=self.n_bits, axis=1)\n        \n        X_out = np.empty((X.shape[0],0))\n        \n        for col in X:\n            # Convert the data type of column into str if int or float\n            if(X[col].dtype == np.float64 or X[col].dtype == np.int64):\n                X[col] = X[col].astype(str)\n                \n            # Transform each feature and convert the data type into float for NaN\n            X_out = np.concatenate((X_out, self.hashing_encoder.fit_transform(X[col].to_numpy()).astype('float')), axis=1)\n            \n        # As hashing encoder will turn NaN into numeric array, reconvert back these values into NaNs\n        X_out[pd.isnull(nan_location_arr)] = np.nan\n        \n        return X_out\n    \n    \nclass hashAggregator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Combine all the 2D arrays of encoded using hash encoding after KNN imputation \n        \n    Parameters\n    ----------\n    n_feas: int\n        Number of features encoded using hash encoding \n    n_bits: int, default=8\n        Number of bits used to represent each nominal feature\n    \"\"\"\n    \n    \n    def __init__(self, n_feas, n_bits):\n        self.n_feas = n_feas\n        self.n_bits = n_bits\n        \n        \n    def fit(self, X, y = None ):\n        \"\"\"\n        Ignored. This exists only for compatibility with \n            :class:`~sklearn.pipeline.Pipeline`.\n        Returns\n        -------\n        self\n        \"\"\"\n        \n        return self\n    \n\n    def transform(self, X):\n        \"\"\"\n        Combine all the 2D arrays of features encoded using hash encoding\n        \n        Parameters\n        ----------\n        X : array, shape [n_samples, n_features*n_bits]\n            The data to combine.\n        Returns\n        -------\n        X_out : 2D array\n            Combined input.\n        \"\"\"\n        X_copy = np.copy(X)\n\n        # Split input array into two array: one containing data encoded using hash encoding, another containing the rest\n        X_copy_non_hash, X_copy_hash = np.split(X_copy, [-self.n_feas*self.n_bits,], axis=1)\n        \n        # Combine all the 2D arrays of features encoded using hash encoding\n        X_copy_hash = X_copy_hash.reshape(X_copy_hash.shape[0], -1, self.n_bits).sum(1)\n        \n        # Concatenate hash and non-hash arrays\n        X_out = np.concatenate((X_copy_non_hash, X_copy_hash), axis=1)\n        \n        return X_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define number of bits for hash encoding\nn_bits = 8\n\n# Define ColumnTransformer to transform columns with different methods\ncolumn_transformer = ColumnTransformer(transformers=[('dis_transformer', 'passthrough', data_types_dict['dis']),\n                                                     ('con_transformer', 'passthrough', data_types_dict['con']),\n                                                     ('ord_transformer', myOrdinalEncoder(), data_types_dict['ord']),\n#                                                      ('nom_one_hot_transformer', myOneHotEncoder(), data_types_dict['nom_one_hot']),\n                                                     ('nom_hash_transformer', myHashingEncoder(n_bits = n_bits), data_types_dict['nom_hash']),\n                                                    ])\n\n\n# Define pipeline for pre-processing data\npreprocessor = Pipeline(steps=[('column_transformer', column_transformer),\n                               ('standard_scaler', StandardScaler()),\n                               ('imputer', KNNImputer(n_neighbors=5, weights='distance')),\n                               ('hash_aggregator', hashAggregator(n_feas=len(data_types_dict['nom_hash']), n_bits=n_bits)),\n                              ])\n\n\ntransformed_data = preprocessor.fit_transform(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning <a class=\"anchor\" id=\"machine-learning\"></a>"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}